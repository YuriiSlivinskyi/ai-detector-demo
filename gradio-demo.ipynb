{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242320193,"sourceType":"kernelVersion"},{"sourceId":414818,"sourceType":"modelInstanceVersion","modelInstanceId":338566,"modelId":359560},{"sourceId":414852,"sourceType":"modelInstanceVersion","modelInstanceId":338566,"modelId":359560}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q gradio\nimport pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nimport os\nimport joblib\nimport gradio as gr\n\n# NLTK for text processing (ensure resources are downloaded as in the main script)\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords # Not directly used in classify_text_pair but part of original context","metadata":{"_uuid":"3ff226ef-9b7f-43df-b6fa-77cd2e14cb1e","_cell_guid":"7774b983-f807-45a0-b873-48e227b82d37","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:20.632699Z","iopub.execute_input":"2025-05-29T10:16:20.633119Z","iopub.status.idle":"2025-05-29T10:16:40.765021Z","shell.execute_reply.started":"2025-05-29T10:16:20.633082Z","shell.execute_reply":"2025-05-29T10:16:40.763798Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ENGLISH_FUNCTION_WORDS = [ # Keep this consistent with the training script\n    'a', 'an', 'the', 'and', 'but', 'or', 'nor', 'for', 'so', 'yet', 'if', 'whether',\n    'in', 'on', 'at', 'by', 'from', 'to', 'with', 'without', 'about', 'above', 'across', 'after',\n    'against', 'along', 'among', 'around', 'as', 'because', 'before', 'behind', 'below',\n    'beneath', 'beside', 'between', 'beyond', 'concerning', 'despite', 'down', 'during',\n    'except', 'inside', 'into', 'like', 'near', 'of', 'off', 'onto', 'out', 'outside',\n    'over', 'past', 'regarding', 'since', 'through', 'throughout', 'toward', 'under',\n    'underneath', 'until', 'unto', 'up', 'upon', 'within',\n    'i', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'yourselves',\n    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n    'we', 'us', 'our', 'ours', 'ourselves', 'they', 'them', 'their', 'theirs', 'themselves',\n    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',\n    'do', 'does', 'did', 'doing', 'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must',\n    'what', 'which', 'who', 'whom', 'whose', 'when', 'where', 'why', 'how',\n    'all', 'any', 'both', 'each', 'either', 'enough', 'every', 'few', 'less', 'little', 'many',\n    'more', 'most', 'much', 'neither', 'no', 'none', 'one', 'other', 'several', 'some', 'such', 'that',\n    'these', 'this', 'those', 'very', 'just', 'not', 'only', 'quite', 'rather', 'too', 'even'\n]\nENGLISH_STOP_WORDS = set(stopwords.words('english')) # Define if used by any get_... function\n\n\ndef preprocess_text_for_stylometry(text):\n    if not isinstance(text, str): return \"\"\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'[^a-z0-9\\s.,!?;:\\'\"-]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef preprocess_text_for_ngrams(text):\n    if not isinstance(text, str): return \"\"\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef get_vocabulary_richness(tokens):\n    if not tokens: return {'ttr': 0.0, 'unique_word_count': 0, 'word_count': 0}\n    word_count = len(tokens)\n    unique_word_count = len(set(tokens))\n    ttr = unique_word_count / word_count if word_count > 0 else 0.0\n    return {'ttr': ttr, 'unique_word_count': unique_word_count, 'word_count': word_count}\n\ndef get_sentence_length_stats(text_original):\n    if not isinstance(text_original, str) or not text_original.strip():\n        return {'avg_sent_len': 0.0, 'std_sent_len': 0.0, 'sentence_count': 0}\n    try:\n        sentences = sent_tokenize(text_original)\n    except LookupError:\n        nltk.download('punkt', quiet=True)\n        sentences = sent_tokenize(text_original)\n    if not sentences: return {'avg_sent_len': 0.0, 'std_sent_len': 0.0, 'sentence_count': 0}\n    sent_lengths = [len(word_tokenize(s)) for s in sentences]\n    return {\n        'avg_sent_len': np.mean(sent_lengths) if sent_lengths else 0.0,\n        'std_sent_len': np.std(sent_lengths) if len(sent_lengths) > 1 else 0.0,\n        'sentence_count': len(sentences)\n    }\n\ndef get_punctuation_stats(text_original):\n    if not isinstance(text_original, str) or not text_original:\n        return {'comma_freq': 0.0, 'period_freq': 0.0, 'question_freq': 0.0, 'exclam_freq': 0.0, 'semicolon_freq': 0.0, 'colon_freq': 0.0, 'hyphen_freq': 0.0, 'quote_freq': 0.0}\n    total_chars = len(text_original) if len(text_original) > 0 else 1\n    return {\n        'comma_freq': text_original.count(',') / total_chars, 'period_freq': text_original.count('.') / total_chars,\n        'question_freq': text_original.count('?') / total_chars, 'exclam_freq': text_original.count('!') / total_chars,\n        'semicolon_freq': text_original.count(';') / total_chars, 'colon_freq': text_original.count(':') / total_chars,\n        'hyphen_freq': text_original.count('-') / total_chars, 'quote_freq': (text_original.count('\"') + text_original.count(\"'\")) / total_chars,\n    }\n\ndef get_function_word_proportions(tokens, function_words_set):\n    if not tokens: return {'func_word_prop': 0.0}\n    func_word_count = sum(1 for token in tokens if token in function_words_set)\n    return {'func_word_prop': func_word_count / len(tokens) if len(tokens) > 0 else 0.0}\n\ndef extract_stylometric_features_single_text(text_original, text_for_stylometry, tokens_for_stylometry):\n    features = {}\n    features.update(get_vocabulary_richness(tokens_for_stylometry))\n    features.update(get_sentence_length_stats(text_original))\n    features.update(get_punctuation_stats(text_original))\n    features.update(get_function_word_proportions(tokens_for_stylometry, set(ENGLISH_FUNCTION_WORDS)))\n    return features\n\n\ndef load_all_artifacts(artifacts_dir=\"/kaggle/input/help-me-2/saved_authorship_artifacts\"):\n    \"\"\"\n    Loads all saved artifacts: vectorizer, feature names, scaler, and models.\n    \"\"\"\n    if not os.path.exists(artifacts_dir):\n        raise FileNotFoundError(f\"Artifacts directory '{artifacts_dir}' not found. Please run the training script first.\")\n\n    loaded_artifacts = {}\n\n    # Load vectorizer\n    vectorizer_path = os.path.join(artifacts_dir, 'char_ngram_vectorizer.joblib')\n    loaded_artifacts['char_ngram_vectorizer'] = joblib.load(vectorizer_path)\n    print(f\"Vectorizer loaded from {vectorizer_path}\")\n\n    # Load feature names\n    feature_names_path = os.path.join(artifacts_dir, 'final_feature_names.joblib')\n    loaded_artifacts['final_feature_names_with_diff'] = joblib.load(feature_names_path)\n    print(f\"Feature names loaded from {feature_names_path}\")\n\n    # Load scaler\n    scaler_path = os.path.join(artifacts_dir, 'scaler.joblib')\n    loaded_artifacts['scaler'] = joblib.load(scaler_path)\n    print(f\"Scaler loaded from {scaler_path}\")\n\n    # Load models\n    loaded_artifacts['models'] = {}\n    model_files = [f for f in os.listdir(artifacts_dir) if f.endswith('_model.joblib')]\n    if not model_files:\n        print(\"Warning: No model files found in the artifacts directory.\")\n    for model_file in model_files:\n        model_name_key = model_file.replace('_model.joblib', '').replace('_', ' ').title() # e.g., \"Logistic Regression\"\n        model_path = os.path.join(artifacts_dir, model_file)\n        try:\n            loaded_artifacts['models'][model_name_key] = joblib.load(model_path)\n            print(f\"Model '{model_name_key}' loaded from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model {model_file}: {e}\")\n            \n    # Derive base stylometric feature names (needed for processing new texts)\n    # These are the names of features *before* \"diff_\" prefix and *excluding* char_ngrams\n    final_names = loaded_artifacts['final_feature_names_with_diff']\n    base_stylometric_names = []\n    for name in final_names:\n        if name.startswith(\"diff_\") and \"char_ngram\" not in name:\n            base_stylometric_names.append(name.replace(\"diff_\", \"\", 1))\n    loaded_artifacts['base_stylometric_feature_names'] = sorted(list(set(base_stylometric_names))) # Ensure uniqueness and order\n\n    return loaded_artifacts\n\n## 2. Function to Classify a Text Pair\ndef classify_text_pair(text1, text2, model, char_ngram_vectorizer, scaler, \n                       base_stylometric_feature_names, final_feature_names_with_diff):\n    \"\"\"\n    Classifies a pair of texts using a loaded model and artifacts.\n    \"\"\"\n    # Preprocess texts\n    text1_original_for_style = text1\n    text2_original_for_style = text2\n    text1_stylometry = preprocess_text_for_stylometry(text1)\n    text2_stylometry = preprocess_text_for_stylometry(text2)\n    text1_stylometry_tokens = word_tokenize(text1_stylometry)\n    text2_stylometry_tokens = word_tokenize(text2_stylometry)\n    text1_for_ngrams = preprocess_text_for_ngrams(text1)\n    text2_for_ngrams = preprocess_text_for_ngrams(text2)\n\n    # Extract stylometric features\n    style_features1_dict = extract_stylometric_features_single_text(text1_original_for_style, text1_stylometry, text1_stylometry_tokens)\n    style_features2_dict = extract_stylometric_features_single_text(text2_original_for_style, text2_stylometry, text2_stylometry_tokens)\n\n    # Create ordered stylometric feature vectors based on base_stylometric_feature_names\n    # This ensures the order matches what was used in training before differencing.\n    vec_style1 = np.array([style_features1_dict.get(name, 0.0) for name in base_stylometric_feature_names])\n    vec_style2 = np.array([style_features2_dict.get(name, 0.0) for name in base_stylometric_feature_names])\n    diff_stylometric_features = np.abs(vec_style1 - vec_style2)\n\n    # Extract character n-gram features\n    char_ngrams1_matrix = char_ngram_vectorizer.transform([text1_for_ngrams])\n    char_ngrams2_matrix = char_ngram_vectorizer.transform([text2_for_ngrams])\n    diff_char_ngrams = np.abs(char_ngrams1_matrix.toarray() - char_ngrams2_matrix.toarray()).flatten()\n\n    # Combine features\n    # The order of concatenation must match the order in final_feature_names_with_diff\n    # We assume stylometric diffs come first, then char_ngram diffs, as in the training script.\n    combined_diff_features_vector = np.concatenate([diff_stylometric_features, diff_char_ngrams])\n    \n    # Reshape for scaler and model (1 sample, N features)\n    combined_diff_features_df = pd.DataFrame([combined_diff_features_vector], columns=final_feature_names_with_diff)\n\n    # Scale features\n    # Note: Use only the columns that the scaler was trained on, if there's a mismatch.\n    # However, final_feature_names_with_diff *should* be what the scaler expects.\n    scaled_features = scaler.transform(combined_diff_features_df)\n\n    # Predict\n    prediction = model.predict(scaled_features)[0]\n    probability = 0.0\n    if hasattr(model, \"predict_proba\"):\n        probs = model.predict_proba(scaled_features)[0]\n        probability = probs[1] if prediction == 1 else probs[0] # Probability of the predicted class\n\n    return prediction, probability, probs","metadata":{"_uuid":"0f622114-128c-47c9-a772-6a6adc749e2c","_cell_guid":"22a76cbd-cc04-4811-bf1f-73389fadfa75","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:40.766963Z","iopub.execute_input":"2025-05-29T10:16:40.767623Z","iopub.status.idle":"2025-05-29T10:16:40.805215Z","shell.execute_reply.started":"2025-05-29T10:16:40.767590Z","shell.execute_reply":"2025-05-29T10:16:40.804364Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_gradio_demo(loaded_artifacts_dict):\n    \"\"\"\n    Creates and launches a Gradio demo for authorship verification.\n    \"\"\"\n    model_choices = list(loaded_artifacts_dict['models'].keys())\n    if not model_choices:\n        print(\"No models loaded. Cannot start Gradio demo.\")\n        return\n\n    char_ngram_vectorizer = loaded_artifacts_dict['char_ngram_vectorizer']\n    scaler = loaded_artifacts_dict['scaler']\n    final_feature_names = loaded_artifacts_dict['final_feature_names_with_diff']\n    base_stylometric_names = loaded_artifacts_dict['base_stylometric_feature_names']\n\n\n    def predict_authorship(text1, text2, model_name_choice):\n        if not text1.strip() or not text2.strip():\n            return \"Error: Both text fields must be filled.\", 0.0\n        \n        selected_model_instance = loaded_artifacts_dict['models'][model_name_choice]\n        \n        # Determine if the chosen model expects scaled data (mimicking training script logic)\n        # This logic should ideally be stored/inferred more robustly\n        scaled_data_models = [\"Logistic Regression\", \"Svm (Linear Kernel)\", \"Svm (Rbf Kernel)\", \"Gaussian Naive Bayes\"] # Title case\n        \n        # The classify_text_pair function now always returns features to be scaled,\n        # and the scaler is applied within it.\n        # The specific model's sensitivity to scaling is handled by whether it was trained on scaled/unscaled data.\n        # For inference, we should always pass the data through the same pipeline (including scaling if a scaler was saved).\n        \n        pred_label, pred_prob, probs = classify_text_pair(\n            text1, text2, selected_model_instance,\n            char_ngram_vectorizer, scaler,\n            base_stylometric_names, final_feature_names\n        )\n    \n        result_text = \"Same Author\" if probs[1] > .6 else \"Different Authors\"\n        \n        # For Gradio label output, return a dictionary for colored labels\n        label_output = {'Same Author': pred_prob - .1,\n                       'Different Author': 1.1 - pred_prob}\n        \n        return label_output, f\"{pred_prob:.4f}\"\n\n\n    iface = gr.Interface(\n        fn=predict_authorship,\n        inputs=[\n            gr.Textbox(lines=10, placeholder=\"Enter first text here...\", label=\"Text 1\"),\n            gr.Textbox(lines=10, placeholder=\"Enter second text here...\", label=\"Text 2\"),\n            gr.Dropdown(choices=model_choices, label=\"Choose Model\", value=model_choices[0] if model_choices else None)\n        ],\n        outputs=[\n            # gr.Textbox(label=\"Result\"), # Simpler text output\n            # gr.Textbox(label=\"Confidence Score\") # Simpler text output for confidence\n            gr.Label(label=\"Authorship Prediction\"), # Using Label for richer output\n            gr.Textbox(label=\"Confidence in Prediction\")\n        ],\n        title=\"Authorship Verification Demo ✍️\",\n        description=\"Enter two texts and select a model to predict if they were written by the same author. \"+ \\\n                    \"The confidence score reflects the model's certainty in the predicted class.\",\n        allow_flagging=\"never\"\n    )\n    \n    print(\"Launching Gradio demo...\")\n    iface.launch()","metadata":{"_uuid":"369d3241-e944-4d4b-9c8a-afb317fdd36d","_cell_guid":"2bca28dc-f85b-4c74-b9c8-771fe6fc985c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:40.806466Z","iopub.execute_input":"2025-05-29T10:16:40.806845Z","iopub.status.idle":"2025-05-29T10:16:40.820441Z","shell.execute_reply.started":"2025-05-29T10:16:40.806813Z","shell.execute_reply":"2025-05-29T10:16:40.819132Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"artifacts = load_all_artifacts()","metadata":{"_uuid":"7d6e9a11-a08b-40a0-b4f1-a307b5c8ec5f","_cell_guid":"d47b9644-b8ff-4732-a921-1a17c3a85ae1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:40.823077Z","iopub.execute_input":"2025-05-29T10:16:40.823443Z","iopub.status.idle":"2025-05-29T10:16:44.307566Z","shell.execute_reply.started":"2025-05-29T10:16:40.823415Z","shell.execute_reply":"2025-05-29T10:16:44.306701Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(artifacts['models'].keys())","metadata":{"_uuid":"8b6d9a3b-61ac-491f-8560-d59ac981def9","_cell_guid":"c8d8fd7a-a212-40fe-b42c-60e4ecf58d73","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:44.308796Z","iopub.execute_input":"2025-05-29T10:16:44.309118Z","iopub.status.idle":"2025-05-29T10:16:44.316812Z","shell.execute_reply.started":"2025-05-29T10:16:44.309093Z","shell.execute_reply":"2025-05-29T10:16:44.315929Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mappings = {v:k for k, v in artifacts['char_ngram_vectorizer'].vocabulary_.items()}","metadata":{"_uuid":"9bc47c8c-163c-45cd-a720-47e59baa2d01","_cell_guid":"0eb9e6c4-9464-4fd4-8391-c62327a0364c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:44.317783Z","iopub.execute_input":"2025-05-29T10:16:44.318105Z","iopub.status.idle":"2025-05-29T10:16:44.339019Z","shell.execute_reply.started":"2025-05-29T10:16:44.318069Z","shell.execute_reply":"2025-05-29T10:16:44.337761Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mappings[869]","metadata":{"_uuid":"cbae3c72-bc14-48a7-b96d-8cf7f572e170","_cell_guid":"cea02d26-73ae-403b-b368-ab39758b0dc1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:44.340138Z","iopub.execute_input":"2025-05-29T10:16:44.340498Z","iopub.status.idle":"2025-05-29T10:16:44.361919Z","shell.execute_reply.started":"2025-05-29T10:16:44.340465Z","shell.execute_reply":"2025-05-29T10:16:44.361048Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"create_gradio_demo(artifacts)","metadata":{"_uuid":"3cbb56b0-531b-411a-9c2b-437f2883d0c2","_cell_guid":"3f9a5e11-126b-4c38-9096-35c8a22e69ea","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:44.362931Z","iopub.execute_input":"2025-05-29T10:16:44.363176Z","iopub.status.idle":"2025-05-29T10:16:46.192261Z","shell.execute_reply.started":"2025-05-29T10:16:44.363155Z","shell.execute_reply":"2025-05-29T10:16:46.191100Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def highlight_characters_in_texts(text1: str, text2: str, char_group: str):\n    \"\"\"\n    Prints two texts, highlighting occurrences of a specified character group with a red background.\n\n    Args:\n        text1 (str): The first text string.\n        text2 (str): The second text string.\n        char_group (str): The group of characters to highlight.\n                          Highlighting is case-insensitive.\n    \"\"\"\n    # ANSI escape codes for red background and resetting color\n    RED_BACKGROUND = \"\\033[41m\"  # 41m for red background\n    RESET = \"\\033[0m\"\n\n    # Check if the character group is empty. If so, print original texts.\n    if not char_group:\n        print(\"--- Text 1 (No highlighting as character group is empty) ---\")\n        print(text1)\n        print(\"\\n--- Text 2 (No highlighting as character group is empty) ---\")\n        print(text2)\n        return\n\n    # Create a case-insensitive replacement function\n    def replace_case_insensitive(text, old, new_format):\n        # Use a regular expression for case-insensitive global replacement\n        import re\n        # Escape special regex characters in the char_group\n        escaped_char_group = re.escape(old)\n        # Replace the matched group with the format, the group itself, and then reset\n        return re.sub(escaped_char_group, f\"{new_format}\\\\g<0>{RESET}\", text, flags=re.IGNORECASE)\n\n    # Highlight text1\n    highlighted_text1 = replace_case_insensitive(text1, char_group, RED_BACKGROUND)\n\n    # Highlight text2\n    highlighted_text2 = replace_case_insensitive(text2, char_group, RED_BACKGROUND)\n\n    print(\"Text 1:\")\n    print(highlighted_text1)\n    print(\"\\nText 2:\")\n    print(highlighted_text2)","metadata":{"_uuid":"975d364d-97b0-46e9-b430-d5cc7a7e76f1","_cell_guid":"09df17c3-4ea8-45c1-9150-a0e5b614f87e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:46.193247Z","iopub.execute_input":"2025-05-29T10:16:46.193522Z","iopub.status.idle":"2025-05-29T10:16:46.201519Z","shell.execute_reply.started":"2025-05-29T10:16:46.193500Z","shell.execute_reply":"2025-05-29T10:16:46.200328Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t1 = \"yeah so let's see here I mean I got into tech because I was obsessed with computers when I was in high school when I thought I would credit them I really wasn't but I kept being interested in it in it so I needed up taking computer science for my undergraduate it was until like my first internship where I realized that this is really boring I started reading or like leads to stuff that I was doing was really boring I started reading a lot more software engineering literature and actually got a lot deeper into I guess the trade then I had initially thought I was going to be and I did I think that's kind of like why I'm here okay as I I still think it's a very interesting way to build out things that are useful for people yeah how I ended up in data is I've always been kind of a database nerd I guess or how how I store data efficiently I think there's been a lot of interesting problems surrounding that and I don't know I just kept digging into those kind of rules my first role out of my undergraduate degree was working on AWS in storage mm-hmm when I was working at a start-up I liked the projects where I got to build out our own data warehouse and then when I applied to next door I found most of my interests being around how to efficiently transform data it's it being something that could be searched and transformed very efficiently so I think that's like largely the journey that I've been through so far\"\nt2 = \"yeah I mean before we dive into the question I guess I have a few questions for you what kind of database yeah okay so with if we don't want to incur any downtime I guess my question to you is what impact with downtime have I would I would actually do this in two phases okay and it also depends on like it can we have like a default value attached to so Postgres allows you to have default values without actually writing the actual call value into the record okay so we can use that as leverage then we don't even have to really backfill we just have to apply our table when our table definition change and then we're good to go so I would do this in phases I think it makes sense to lock out chunks of Records and update those values but let's initially just have them per second so let's say if there's a billion rows then let's say that there gets like a million rows added per day and then let's say like if we extrapolate from that like probably like like a couple like a thousand records per second that's probably not right but let's just say a thousand records per second okay yeah so one way to kind of do this is to create a table that is identical to the other table but with an extra record and have all new records write to that new table and then we'll slowly start copying data from that old table to the new table with whatever default value that we want to give at that table and when we're reading from that table we have to read from both tables for the short for the short term okay eventually after all of the old records are copied over to the new table we can drop the old table and move on with our lives and then we have we effectively have our new column with no downtime um I think the questions that were asked were intentionally left vague no I definitely have to probe and get a better understanding of like where our constraints lives yep so that I could give a good ish answer I mean lemme answer so kind of hand wavy if I were being quite honest but I think like I think high level I hit them okay I thought that one's hard to answer actually um I tend to play it by ear so I think if you ask too many questions then you're not doing any thinking or any Discovery on your own yeah which doesn't but well in debrief so it's good to have some ideas talk aloud on those kind of ideas and even just ask for feedback on what the interviewers thinks of like your idea yep um and I think that's good I think I think if you listen you also need to listen to the interviewer and pick up on any sort of hints that they may give you so when you said do things letter by letter immediately clicked in my head that was a hint yeah and if I pick up on the hand then the interviewer knows that I picked up on the hand and can probably think or I think the interview will think like okay this person probably knows yeah it what's going on here and what I was thinking in terms of structure I think it makes a lot of sense\"\na = highlight_characters_in_texts(t1, t2, mappings[2692])","metadata":{"_uuid":"620f70da-524d-4994-a5f0-00983bd5bac0","_cell_guid":"0006d79b-4bbf-4a52-821a-531cde9927c6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-29T10:16:46.204489Z","iopub.execute_input":"2025-05-29T10:16:46.204981Z","iopub.status.idle":"2025-05-29T10:16:46.229298Z","shell.execute_reply.started":"2025-05-29T10:16:46.204947Z","shell.execute_reply":"2025-05-29T10:16:46.228347Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}